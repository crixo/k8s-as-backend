# K8sAsBackend Operator

## Dependencies
you have to install the following dependencies

- docker engine (eg. [docker for desktop](https://docs.docker.com/install/)  for windows and mac users)
- [kind](https://kind.sigs.k8s.io/) The *right way* to have k8s locally - v0.6.0 w/ k8s image v1.16.3
- [golang 1.13`*`](https://golang.org/doc/install) *THE* k8s programming language
- [operator-SDK`*`](https://github.com/operator-framework/operator-sdk) - v0.15.2 The framework selected to build k8s operator
- [envsubst](https://github.com/a8m/envsubst) (GNU gettext-runtime) 0.20.2 - for templating/env var replacing within plain yaml files.
para- [jq](https://stedolan.github.io/jq/) version 1.6 - lightweight and flexible command-line JSON processor.

`*` required only to simplify development and debugging activities. The final solution, as per [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/), runs within a docker container inside the k8s cluster. 

## Demo steps
Start from the repo root, not form this operator folder. Each step starts from repo root

- Create cluster

A 3 node cluster (1 master, 2 workers) will be created with a nginx-controller configured and tested w/ dummy apps (foo and bar).  
The nginx-controller uses ports 80 and 443: make sure you do not have other services currently running on those port otherwise the installation will fail.
```
cd kind
# proposed default cluster name works just fine
sh create-cluster.sh 
# wait until done at least the cluster creation itself (~1 minute)
# nginx-ingress-controller installation&checks may take a bit longer
cd ..
```

- Deploy operator **CustomResourceDefinition**

```
cd operator
kubectl apply -f deploy/crds/k8s-as-backend.example.com_k8sasbackends_crd.yaml
cd ..
```

- Start the operator locally

Use a dedicated bash terminal. 
All the docker images currently part of the workload are stored on the public docker registry so kind cluster will be able to get it but you need an internet connection. To speed up the operator demo itself, I suggest you to use the pre-load images script.
```
#proposed default cluster name works just fine of course it has to match w/ the cluster creation step.
sh kind/preload-image.sh
```

Before running the app against a new cluster, make sure no PEM files are still present in the $TMPDIR folder (default location).  
You need at least a set of new PEM anytime you create a new k8s cluster. PEM clean up will be addressed in future releases.

- Create a dedicated namespace for the demo of the running operator
```
kubectl create namespace operator-running
```

- Start the operator app. The app will be in interactive mode sending logs to stdout
```
cd operator
operator-sdk run --local --namespace=operator-running
```
You'll get some initial log reflecting the current operator configuration, then app waits for works to do through CR, the next step.

- Switch and persist the context using the demo namespace
```
kubectl config set-context --current --namespace operator-running
```

- Deploy your first operator **CustomResource**
Open a dedicated terminal tab pointing to repo root.
```
cd operator
kubectl apply -f deploy/crds/kab01.yaml
```
Go back to the terminal/tab where is running the operator app. You should see a lot of logs describing the tasks accomplished and the deployed resources. 

- Verify the CR has been updated by the deployment workflow adding the PEM just created or founded.
```
kubectl get k8sasbackends.k8s-as-backend.example.com  kab01 -o yaml
```

- Verify main secondary resources has been successfully deployed
```
kubectl get all
```

- Verify cluster-wide resources has been successfully deployed
```
kubectl get validatingwebhookconfigurations
kubectl get crd
```

- Open the browser and test the app

Use the [TodoApp](http://localhost/operator-running/kab01/todo-app/swagger-ui/index.html) that expose your business app. Create some todo and browse it through the swagger ui. *code* property value has to be unique within your app scope otherwise the request will fail.

- Check containers log to ensure the full workflow
```
kubectl get po
# check admission controller logs
kubectl logs kab01-todos-webhook-server-USE_YOUR_DEPLOYMENT_UNIQUE_IDENTIFIER
# check infomer logs
kubectl logs kab01-todo-app-USE_YOUR_DEPLOYMENT_UNIQUE_IDENTIFIER -c informer
```

## Check k8s events generated by the todo-informer
```
kubectl get events -n operator-running --sort-by=.metadata.creationTimestamp
```

## Browse the operator instance via api-server
Start api-server proxy into a dedicated terminal tab
```
kubectl proxy
```
then you can get [all the endpoints/paths exposed by the api-server](http://127.0.0.1:8001).  
The paths list includes k8s built-in paths and custom paths created through CRDs.

You can browse your CRDs definitions and CR instances through api server:
- [operator CRD](http://127.0.0.1:8001/apis/k8s-as-backend.example.com/v1alpha1)
- [operator CRs/instances](http://127.0.0.1:8001/apis/k8s-as-backend.example.com/v1alpha1/k8sasbackends) 
- [operator CRs/instances by namespace](http://127.0.0.1:8001/apis/k8s-as-backend.example.com/v1alpha1/namespaces/operator-running/k8sasbackends)
- [operator CR/instance by namespace and name](http://127.0.0.1:8001/apis/k8s-as-backend.example.com/v1alpha1/namespaces/operator-running/k8sasbackends/kab01)

## Scale the solution through the operator CR
```
kubectl edit k8sasbackends.k8s-as-backend.example.com  kab01 -o yaml
```

- Clean up everything
```
cd operator
kubectl delete -f deploy/crds/kab01.yaml
```
At the present cluster-wide resource are not removed if no longer needed

## Full demo using e2e testing framework
use the following script to create the cluster and/or deploy the workload using the operator
```
sh run-all.sh
```

## Build operator image

- Build the operator image
```
cd operator
operator-sdk build crixo/k8s-as-backend-operator:v0.0.0
sed 's|REPLACE_IMAGE|crixo/k8s-as-backend-operator:v0.0.0|g' deploy/operator.yaml.template > deploy/operator.yaml
docker push crixo/k8s-as-backend-operator:v0.0.0
```

- Deploy operator CRD 
```
kubectl apply -f deploy/crds/k8s-as-backend.example.com_k8sasbackends_crd.yaml
```

- Create and configure namespace for this demo
```
kubectl create namespace operator-in-cluster
kubectl config set-context --current --namespace operator-in-cluster
```

- Deploy RBAC resources for the operator app running in cluster
```
kubectl create -f deploy/service_account.yaml

#TODO: replace hardcoded ns
kubectl create -f deploy/cluster_role_binding_cluster_admin.yaml
```

- Deploy the operator app within the cluster
```
kubectl create -f deploy/operator.yaml
```

- Deploy the operator CR IOW your solution/app instance
```
kubectl apply -f deploy/crds/kab01.yaml
```

- Check the status of the operator app deployment
```
kubectl get pod
```

browse the [todo app](http://localhost/operator-in-cluster/kab01/todo-app/swagger-ui/index.html) deployed by the operator running in cluster

- [operator CR/instance by namespace and name](http://127.0.0.1:8001/apis/k8s-as-backend.example.com/v1alpha1/namespaces/operator-in-cluster/k8sasbackends/kab01)

- Side by Side apps: compare app managed by [local operator](http://localhost/operator-running/kab01/todo-app/swagger-ui/index.html) vs app managed by [operator in cluster](http://localhost/operator-in-cluster/kab01/todo-app/swagger-ui/index.html)

## Deploy operator on AKS

- Create AKS cluster
starting from the repo root
```
cd aks
DNSNAME="operator-demo"
sh cluster-creation.sh "cluster-operator-demo" $DNSNAME
sh aks-operator-deploy.sh "operator-in-cluster" $DNSNAME
```

- browse the [todo app](https://operator-demo.westeurope.cloudapp.azure.com/operator-in-cluster/kab01/todo-app/swagger-ui/index.html) in AKS


## Notes
The images on docker hub may not be up to date. Use the specific folder at root level to build the latest version of the images.
